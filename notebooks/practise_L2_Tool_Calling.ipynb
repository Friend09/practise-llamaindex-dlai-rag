{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Tool Calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE SIMPLE TOOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def mystery(x: int, y: int) -> int:\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='121', sources=[ToolOutput(content='121', tool_name='mystery', raw_input={'args': (), 'kwargs': {'x': 2, 'y': 9}}, raw_output=121, is_error=False)], source_nodes=[], is_dummy_stream=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm_llamaindex = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm_llamaindex.predict_and_call(\n",
    "    [add_tool, mystery_tool],\n",
    "    # \"What is the output of adding 2 and 9\",\n",
    "    \"Tell me the output of the mystery function on 2 and 9\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE AN AUTO-RETRIEVAL TOOL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data as documents\n",
    "pdf_path = \"../files/metagpt.pdf\"\n",
    "\n",
    "from llama_index.legacy import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document chunks/nodes\n",
    "from llama_index.legacy.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node embeddings, create query engine\n",
    "from llama_index.legacy import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '7', 'file_name': 'metagpt.pdf', 'file_path': '../files/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-10', 'last_modified_date': '2024-06-08', 'last_accessed_date': '2024-06-22'}\n",
      "{'page_label': '23', 'file_name': 'metagpt.pdf', 'file_path': '../files/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-10', 'last_modified_date': '2024-06-08', 'last_accessed_date': '2024-06-22'}\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are some high-level results of MetaGPT?\")\n",
    "\n",
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'metagpt.pdf',\n",
       " 'file_path': '../files/metagpt.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 16911937,\n",
       " 'creation_date': '2024-06-10',\n",
       " 'last_modified_date': '2024-06-08',\n",
       " 'last_accessed_date': '2024-06-22'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view available metadata filters\n",
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query engine w/ metadata filters\n",
    "from llama_index.legacy.vector_stores import MetadataFilters\n",
    "\n",
    "# define filter to seach only page label == 2 pages\n",
    "query_engine_w_filters = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\n",
    "                \"key\": \"page_label\",\n",
    "                \"value\": \"2\",\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '../files/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-06-10', 'last_modified_date': '2024-06-08', 'last_accessed_date': '2024-06-22'}\n"
     ]
    }
   ],
   "source": [
    "response_using_filters = query_engine_w_filters.query(\n",
    "    \"What are some high-level results of MetaGPT?\"\n",
    ")\n",
    "\n",
    "for n in response_using_filters.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the auto-retrieval tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query the vector engine and return the response\n",
    "from typing import List\n",
    "from llama_index.legacy.vector_stores import FilterCondition  # to combine diff filters\n",
    "\n",
    "\n",
    "def vector_query(query: str, page_numbers: List[str]) -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "\n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search over all pages. Else, filter by the set of specified pages.\n",
    "    \"\"\"\n",
    "\n",
    "    # define the metadata filter to use\n",
    "    metadata_dicts = [\n",
    "        {\n",
    "            \"key\": \"page_label\",\n",
    "            \"value\": page,\n",
    "        }\n",
    "        for page in page_numbers\n",
    "    ]\n",
    "\n",
    "    # define the query vector engine\n",
    "    query_engine_w_filters = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR,  # if there are more than one filter in the metadata_dicts use OR logic\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # get response from the engine\n",
    "    response_using_filters = query_engine_w_filters.query(query)\n",
    "    return response_using_filters\n",
    "\n",
    "\n",
    "tool_vector_query = FunctionTool.from_defaults(\n",
    "    name=\"tool_vector_query\", fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_vector_query with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It stands out in handling higher levels of software complexity and offering extensive functionality, demonstrating a 100% task completion rate in experimental evaluations.\n"
     ]
    }
   ],
   "source": [
    "# define an llm\n",
    "llm_llamaindex = OpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "query = \"What are the high-level results of MetaGPT as described on page 2?\"\n",
    "\n",
    "response = llm_llamaindex.predict_and_call(\n",
    "    [tool_vector_query],  # tool to use\n",
    "    query,  # query for tools to use\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'604e1c0a-926a-4b60-82a5-12820ce0b679': {'creation_date': '2024-06-10',\n",
      "                                          'file_name': 'metagpt.pdf',\n",
      "                                          'file_path': '../files/metagpt.pdf',\n",
      "                                          'file_size': 16911937,\n",
      "                                          'file_type': 'application/pdf',\n",
      "                                          'last_accessed_date': '2024-06-22',\n",
      "                                          'last_modified_date': '2024-06-08',\n",
      "                                          'page_label': '2'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "for i in range(0, (len(response.sources))):\n",
    "    pp(response.sources[i].raw_output.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy import SummaryIndex\n",
    "from llama_index.legacy.tools import QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary embeddings\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the summary engine as a tool\n",
    "tool_summary = QueryEngineTool.from_defaults(\n",
    "    name=\"tool_summary\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\"Useful if you want to get a summary of MetaGPT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the MetaGPT comparisons with ChatDev described on page 8?'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the MetaGPT comparisons with ChatDev described on page 8?\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_vector_query with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev on the SoftwareDev dataset in various aspects. For example, MetaGPT achieves a higher score in executability, takes less time for execution, uses more tokens but requires fewer tokens to generate one line of code compared to ChatDev. Additionally, MetaGPT surpasses ChatDev in code statistic metrics and human revision cost, showcasing the advantages of utilizing SOPs in collaborative efforts between multiple agents.\n"
     ]
    }
   ],
   "source": [
    "response = llm_llamaindex.predict_and_call(\n",
    "    [tool_vector_query, tool_summary],\n",
    "    query,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper does not discuss the impact of climate change on biodiversity and ecosystems.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'649836fa-0944-449c-8674-a53b34d9f066': {'creation_date': '2024-06-10',\n",
      "                                          'file_name': 'metagpt.pdf',\n",
      "                                          'file_path': '../files/metagpt.pdf',\n",
      "                                          'file_size': 16911937,\n",
      "                                          'file_type': 'application/pdf',\n",
      "                                          'last_accessed_date': '2024-06-22',\n",
      "                                          'last_modified_date': '2024-06-08',\n",
      "                                          'page_label': '8'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "for i in range(0, (len(response.sources))):\n",
    "    pp(response.sources[i].raw_output.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_summary with args: {\"input\": \"The paper discusses the impact of climate change on biodiversity and ecosystems.\"}\n",
      "=== Function Output ===\n",
      "The paper does not discuss the impact of climate change on biodiversity and ecosystems.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [tool_vector_query, tool_summary], \"What is a summary of the paper?\", verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
